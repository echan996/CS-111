Lab 4
CS111, Winter 2016

Eric Chan
504447283
echan996@gmail.com

Elise Yuen
604418732
ejyuen@att.net

************************
         PART 1
************************

QUESTIONS 1.1
1) In our program, a parent thread will set up many child threads, each requiring a
set up time x. Once the child is set up, it will run in time y. However, if the
parent sets up the child in time x < y (the previous child thread’s run time), then 
two threads will run concurrently and compete for the same resources, resulting in a
race condition. The possibility of race conditions increases with greater numbers of
threads.

We must have at least two threads to have a race condition. Even then, two threads
introduces the possibility of having a race condition, but does not guarantee an
error in the result. Thus, we will look at situations with more threads so that we
guarantee this result error. We used 10 threads as our benchmark because it failed
often enough in the presence of possible race conditions. 

The minimum number of iterations that we found to consistently result in a failure
reflects an approximation of how long the Linux server we used took to set up a
thread.

2) A small number of iterations often represents a period of time less than the time
it takes for the parent thread to set up a child. So, the chances of race
conditions, and thus failure, is less because the two threads don’t intersect.
An iteration value that sometimes results in error is a number that adds up to a
time interval that is close to the thread set up time, making it more likely for
errors, but not guaranteed.

QUESTIONS 1.2
1) The average cost per operation drops with increasing iterations because a lot of
the total time is spent creating the threads. As we add more iterations, this adds
very little to the overall time, so the overall time doesn’t change very much, but
it is divided by a larger number with an increasing number of iterations. Thus, each
operation becomes cheaper and cheaper because we divide the child thread set up time
(plus the little time that each of the iterations add) amongst more operations.

2) We can find the “correct” cost by increasing the number of operations until we 
find the limit. This represents the “correct” cost because the set up time becomes 
negligible when divided amongst so many iterations. 

3) When yield is enable, we have a much larger overall time, indicating that the
operations are much slower when yield is on that when it is off. Yield means that
the thread gives up the CPU in order to let another thread have its turn to run. So,
we can see that this overall time difference is due to the time spent context
switching between threads each time yield is called.

4) We should not trust the resulting timings using yield to give us reliable or 
correct results. Since so much of the program is spent doing context switches, we
can’t use this value as a valid indicator of how long each operation actually takes.
Context switching isn’t considered useful work, so we should not factor this into 
our overall time when calculating the time for the actual operations in themselves.
Thus, we can get best “correct” results regarding operations when yield is not used.


QUESTIONS 1.3
1) Firstly, there are not many threads competing for the lock, so the slowdown will not be
as noticable. There aren't many computations for the lock for a low number of threads, so it 
won't take very long to get the lock and the difference won't be very noticable. The price of
maintaining a lock is cheap; the main cost comes from threads 

2) The protected operations will slow down because it only allows for one thread at a time
to access the protected variable. Because of this, each thread will have access to the processors
for a shorter amount of time and as a result, will have higher on average times for each operation.

3) Spin locks are more expensive because when the thread isn't running, it will be busy-waiting 
rather than being blocked. Since in a large number of threads, all but one will be busy-waiting, thus
a large section of time will be spent on waiting on access to a protected variable in this case. 



************************
         PART 2
************************

QUESTIONS 2.1
1) As seen in the graph, the cost per iteration starts very high, drops dramatically, and begins 
to increase again towards of our data collection. Specifically, we got about 81 ns for 50 iterations,
9 ns for 500 iterations, 7 ns for 5000 iterations, and 13 ns for 50000 iterations. The decrease in time
in the beginning of the trend is a bit confusing. We can theorize that this is due to overhead,
particularly in the overhead of creating threads. When we think of it this way, it makes sense because
this is the same kind of reasoning we saw in part 1 where the cost per operation decreases because the
thread creation time is spread amongst more operations.

The increase also makes sense. We begin to see an increase in time because we have hit a point
where the larger iterations (which correlates to more elements) are needing to traverse more elements,
which will obviously take longer amounts of time. If we want to fix this, we should use lists. This
will divide the work so that the time will stay low and we will no longer see the increasing trend.

QUESTIONS 2.2
1) For this part of the assignment, I used the following command in order to collect data:
$ ./sltest —-iteration=1000 —-threads=# —sync=m —yield=s
In this case, we varied the number of threads since we were supposed to graph different values
of threads. Comparing the change in time as compared to the change in the number of iterations, we
can see that they grow in an exponential trend. Compared to part 1, this is a similar trend as they
both increase as the number of threads increases. This is because since there is only one lock, 
more threads will need to share this lock, meaning that they will each be waiting longer as the
number of threads increases. Overall, sltest will take longer than addtest because of the
complexity of the thread actions. addtest has the simple computation of adding numbers, while 
sltest demands traversing across many pointers several times.


QUESTIONS 2.3
1) When there are multiple lists, there are more locks available for each of the threads to use, s
they will spend less time wasted waiting around. Thus, we see faster results.

2) We can see a more interesting number when we increase the threads per list because the threads
are doing more useful work since, once again, they will not be waiting for a lock to open up. If
there are more lists, there will be a lot of wasted waiting time for the single lock that all threads
share.


************************
         PART 3
************************

QUESTIONS 3.1
1) The mutex must be held otherwise a thread may change the shared data between the time the condtion
is checked and the time the thread begins to wait. Since pthread_cond_wait must be used with a
conditional check for proper usage, this conditional may be subject to a race condition if the thread
calling pthread_cond_wait does not have the mutex.

2) The waiting thread MUST release the mutex if it is blocked because otherwise, the program will
hang, as the queue of threads waiting on this mutex will never get it, since it is being held by
a blocked thread that is waiting for its turn to be woken up and given the mutex it already
holds. It makes sense that the thread should release the mutex because it is essentially saying
"I don't need this lock, come back to me when my condition is valid so I can do my work."

3) The point of pthread_cond_wait is to suspend synchronous execution until a certain condition is met.
As a result, it must restore the state of the thread to its condition BEFORE pthread_cond_wait is called.
If the mutex is not reacquired, we will have the risk of race conditions because shared variables will be
able to be accessed by other threads.

4) The manner in which pthread_cond_wait is normally used is shown as below:
while ( cond is false ) {
        pthread_cond_wait( &cond, &mutex );
}
If the mutex is released right before the call to pthread_cond_wait, another thread may modify cond, making
it true. In this case, we not want to make the thread wait, 	as its condition is satisfied. As a result, the
release of mutex must be done within the pthread_cond_wait call.

5) It must be in kernel mode. Since we require for the function to atomic, there is no way to guarantee for this
correctness if we run purely in user mode, as we could be interrupted at any point.